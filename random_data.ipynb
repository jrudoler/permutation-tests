{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import multivariate_normal, invwishart\n",
    "def random_data_gen(n_samples=1000, n_feats=10, maha=1.0, psi_diag=1.0, psi_offdiag=0., ddof=150, class_ratio=0.5, seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    ## initialize multivariate normal dist with normally distributed means and covariance\n",
    "    ## drawn from an inverse wishart distribution (conjugate prior for MVN)\n",
    "    norm_means_a = np.random.randn(n_feats)\n",
    "    norm_means_b = np.zeros_like(norm_means_a)\n",
    "    psi = psi_diag * np.eye(n_feats) + psi_offdiag * ~np.eye(n_feats).astype(bool)\n",
    "    nu = n_feats + ddof\n",
    "    wishart_cov = invwishart(nu, psi).rvs()\n",
    "    ## specify the mahalanobis distance between the two distributions\n",
    "    dist = mahalanobis(norm_means_a, norm_means_b, wishart_cov)\n",
    "    norm_means_a = norm_means_a * (maha / dist)\n",
    "    assert np.isclose(mahalanobis(norm_means_a, norm_means_b, wishart_cov), maha)\n",
    "    ## multivariate normal distributions with different means and equal variances\n",
    "    mvn_a = multivariate_normal(mean=norm_means_a, cov=wishart_cov)\n",
    "    mvn_b = multivariate_normal(mean=norm_means_b, cov=wishart_cov)\n",
    "    ## not used, but compute correlations\n",
    "    corr = (D:=np.diag(1/np.sqrt(np.diag(wishart_cov)))) @ wishart_cov @ D\n",
    "    ## generate data samples from a multivariate normal\n",
    "    data = np.vstack([mvn_a.rvs(int(n_samples*class_ratio)), mvn_b.rvs(n_samples - int(n_samples*class_ratio))])\n",
    "    labels = np.arange(len(data))<int(n_samples*class_ratio)\n",
    "    return data, labels\n",
    "#     idx = np.random.choice(np.arange(n_samples), n_samples, replace=False)\n",
    "#     return data[idx], labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5.60382192e+00, -2.00184839e+00, -1.76000226e+00, ...,\n",
       "         -2.49541696e+00,  1.07216939e+00, -9.22616432e-01],\n",
       "        [ 5.49165612e+00, -2.05865832e+00, -1.84694233e+00, ...,\n",
       "         -2.55926190e+00,  1.06162122e+00, -8.08179665e-01],\n",
       "        [ 5.63523162e+00, -2.13963632e+00, -1.86029316e+00, ...,\n",
       "         -2.57952799e+00,  1.12648119e+00, -8.66350227e-01],\n",
       "        ...,\n",
       "        [-5.07071616e-02,  3.53242082e-03,  4.07076712e-02, ...,\n",
       "         -8.49180686e-02, -8.76625594e-02,  2.72813781e-02],\n",
       "        [ 3.34065677e-02, -1.09287952e-01, -4.01749074e-02, ...,\n",
       "         -2.99504685e-02, -7.43017354e-02, -3.50626539e-02],\n",
       "        [-1.04111421e-01, -4.46627065e-02,  3.69168349e-03, ...,\n",
       "          1.23519444e-01,  4.22717064e-02, -3.64803443e-03]]),\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = random_data_gen(n_samples=1000, n_feats=10, maha=1., psi_diag=1., seed=1)\n",
    "data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"random_data_X.npy\", data)\n",
    "np.save(\"random_data_y\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_invwish(psi, dof):\n",
    "    n_feats = len(psi)\n",
    "    return psi / (dof-n_feats-1)\n",
    "\n",
    "def Var_invwish(psi, dof):\n",
    "    p = len(psi)\n",
    "    Var = np.empty((p, p))\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            Var[i][j] = (dof-p+1) * psi[i][j]**2 + (dof-p-1) * psi[i][i]*psi[j][j] \n",
    "    Var /= (dof-p)*(dof-p-1)**2*(dof-p-3)\n",
    "    return Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Cov:\n",
      "[[0.22222222 0.         0.         0.         0.        ]\n",
      " [0.         0.22222222 0.         0.         0.        ]\n",
      " [0.         0.         0.22222222 0.         0.        ]\n",
      " [0.         0.         0.         0.22222222 0.        ]\n",
      " [0.         0.         0.         0.         0.22222222]]\n",
      "Expected Corr:\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "Variance of Cov:\n",
      "[[0.01410935 0.00634921 0.00634921 0.00634921 0.00634921]\n",
      " [0.00634921 0.01410935 0.00634921 0.00634921 0.00634921]\n",
      " [0.00634921 0.00634921 0.01410935 0.00634921 0.00634921]\n",
      " [0.00634921 0.00634921 0.00634921 0.01410935 0.00634921]\n",
      " [0.00634921 0.00634921 0.00634921 0.00634921 0.01410935]]\n"
     ]
    }
   ],
   "source": [
    "def covariance_invwishart(diag=1., offdiag=0., ddof=4, p=5):\n",
    "    psi = diag*np.eye(5) + offdiag * np.ones((p, p))\n",
    "    expected = E_invwish(psi, p+ddof)\n",
    "    variance = Var_invwish(psi, p+ddof)\n",
    "    return expected, variance \n",
    "\n",
    "expected, variance = covariance_invwishart(2, 0, ddof=10)\n",
    "print(f\"Expected Cov:\\n{expected}\")\n",
    "corr = (D:=np.diag(1/np.sqrt(np.diag(expected)))) @ expected @ D\n",
    "print(f\"Expected Corr:\\n{corr}\")\n",
    "print(f\"Variance of Cov:\\n{variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'psi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_104269/2320953709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miw_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvwishart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpsi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0miw_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'psi' is not defined"
     ]
    }
   ],
   "source": [
    "iw_dist = invwishart(df=9, scale=psi)\n",
    "iw_dist.rvs(100000).var(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = random_data_gen(n_samples=1000, n_feats=10, maha=1., psi_diag=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.11337559e+01, -3.08223678e+00,  7.00927508e+00,\n",
       "        -1.09136845e-01, -2.62665209e+00,  1.57397862e+01,\n",
       "        -2.12814900e+00, -2.61325522e+00, -2.19929252e-01,\n",
       "         3.81866761e+00],\n",
       "       [-3.08223678e+00,  4.56311417e-01, -1.02238632e+00,\n",
       "         1.62698098e-02,  3.83464766e-01, -2.29681948e+00,\n",
       "         3.09756520e-01,  3.81738315e-01,  3.24989558e-02,\n",
       "        -5.57379061e-01],\n",
       "       [ 7.00927508e+00, -1.02238632e+00,  2.33222478e+00,\n",
       "        -3.66479657e-02, -8.69779746e-01,  5.22311878e+00,\n",
       "        -7.07011244e-01, -8.67886770e-01, -7.28119600e-02,\n",
       "         1.26627167e+00],\n",
       "       [-1.09136845e-01,  1.62698098e-02, -3.66479657e-02,\n",
       "         5.85556937e-03,  1.31990962e-02, -8.20331257e-02,\n",
       "         1.01523323e-02,  1.41373474e-02,  1.20997425e-03,\n",
       "        -1.95500377e-02],\n",
       "       [-2.62665209e+00,  3.83464766e-01, -8.69779746e-01,\n",
       "         1.31990962e-02,  3.33782405e-01, -1.95560659e+00,\n",
       "         2.63479777e-01,  3.24377059e-01,  2.87234823e-02,\n",
       "        -4.74337538e-01],\n",
       "       [ 1.57397862e+01, -2.29681948e+00,  5.22311878e+00,\n",
       "        -8.20331257e-02, -1.95560659e+00,  1.17358577e+01,\n",
       "        -1.58693939e+00, -1.94762669e+00, -1.65206529e-01,\n",
       "         2.84462173e+00],\n",
       "       [-2.12814900e+00,  3.09756520e-01, -7.07011244e-01,\n",
       "         1.01523323e-02,  2.63479777e-01, -1.58693939e+00,\n",
       "         2.21327700e-01,  2.63032505e-01,  2.23036888e-02,\n",
       "        -3.83994985e-01],\n",
       "       [-2.61325522e+00,  3.81738315e-01, -8.67886770e-01,\n",
       "         1.41373474e-02,  3.24377059e-01, -1.94762669e+00,\n",
       "         2.63032505e-01,  3.29493613e-01,  2.67476232e-02,\n",
       "        -4.72298495e-01],\n",
       "       [-2.19929252e-01,  3.24989558e-02, -7.28119600e-02,\n",
       "         1.20997425e-03,  2.87234823e-02, -1.65206529e-01,\n",
       "         2.23036888e-02,  2.67476232e-02,  8.91287865e-03,\n",
       "        -3.97533331e-02],\n",
       "       [ 3.81866761e+00, -5.57379061e-01,  1.26627167e+00,\n",
       "        -1.95500377e-02, -4.74337538e-01,  2.84462173e+00,\n",
       "        -3.83994985e-01, -4.72298495e-01, -3.97533331e-02,\n",
       "         6.96300581e-01]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.96151568,  7.0445802 , -1.29205562,  1.92744237,  5.91554689],\n",
       "       [ 7.0445802 , 10.03374466, -1.83804783,  2.74286713,  8.42024327],\n",
       "       [-1.29205562, -1.83804783,  0.34757278, -0.5015138 , -1.54477097],\n",
       "       [ 1.92744237,  2.74286713, -0.5015138 ,  0.76392008,  2.30213762],\n",
       "       [ 5.91554689,  8.42024327, -1.54477097,  2.30213762,  7.08354046]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_covar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.32461816e-02, -7.42721852e-04, -9.14612845e-04,\n",
       "         1.10285195e-03,  4.55117617e-05],\n",
       "       [-7.42721852e-04,  1.31646218e-02,  1.34630179e-03,\n",
       "         2.31997711e-03,  1.35394691e-04],\n",
       "       [-9.14612845e-04,  1.34630179e-03,  7.69575346e-03,\n",
       "         1.62357035e-03, -1.15958329e-03],\n",
       "       [ 1.10285195e-03,  2.31997711e-03,  1.62357035e-03,\n",
       "         1.04366977e-02, -5.15768534e-04],\n",
       "       [ 4.55117617e-05,  1.35394691e-04, -1.15958329e-03,\n",
       "        -5.15768534e-04,  8.58132633e-03]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_covar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_covar= np.stack([np.cov(random_data_gen(n_samples=200, n_feats=5, maha=1., psi_diag=1., ddof=100)[0][:100].T) for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corr = np.stack([(D:=np.diag(1/np.sqrt(np.diag(wishart_cov)))) @ wishart_cov @ D for wishart_cov in data_covar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.        , -0.26030399, -0.28292655, -0.26624259,\n",
       "         -0.27525976],\n",
       "        [-0.26030399,  1.        , -0.26971977, -0.26895583,\n",
       "         -0.26083137],\n",
       "        [-0.28292655, -0.26971977,  1.        , -0.27437894,\n",
       "         -0.27280784],\n",
       "        [-0.26624259, -0.26895583, -0.27437894,  1.        ,\n",
       "         -0.27590618],\n",
       "        [-0.27525976, -0.26083137, -0.27280784, -0.27590618,\n",
       "          1.        ]],\n",
       "\n",
       "       [[ 1.        ,  0.27354688,  0.27247805,  0.26425711,\n",
       "          0.25993115],\n",
       "        [ 0.27354688,  1.        ,  0.2581902 ,  0.26684015,\n",
       "          0.27829434],\n",
       "        [ 0.27247805,  0.2581902 ,  1.        ,  0.27000853,\n",
       "          0.27974811],\n",
       "        [ 0.26425711,  0.26684015,  0.27000853,  1.        ,\n",
       "          0.26837825],\n",
       "        [ 0.25993115,  0.27829434,  0.27974811,  0.26837825,\n",
       "          1.        ]]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(data_corr, q=[.025, .975], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.        , -0.16903361, -0.18102743, -0.17935758,\n",
       "         -0.17772066],\n",
       "        [-0.16903361,  1.        , -0.17282183, -0.18994219,\n",
       "         -0.18048465],\n",
       "        [-0.18102743, -0.17282183,  1.        , -0.17753612,\n",
       "         -0.18197967],\n",
       "        [-0.17935758, -0.18994219, -0.17753612,  1.        ,\n",
       "         -0.18635543],\n",
       "        [-0.17772066, -0.18048465, -0.18197967, -0.18635543,\n",
       "          1.        ]],\n",
       "\n",
       "       [[ 1.        ,  0.1825135 ,  0.1924824 ,  0.17781244,\n",
       "          0.17790806],\n",
       "        [ 0.1825135 ,  1.        ,  0.17543695,  0.17805637,\n",
       "          0.18555939],\n",
       "        [ 0.1924824 ,  0.17543695,  1.        ,  0.17391113,\n",
       "          0.18311711],\n",
       "        [ 0.17781244,  0.17805637,  0.17391113,  1.        ,\n",
       "          0.1822637 ],\n",
       "        [ 0.17790806,  0.18555939,  0.18311711,  0.1822637 ,\n",
       "          1.        ]]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(data_corr, q=[.1, .9], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01,  0.  , -0.  , -0.  ,  0.  ],\n",
       "       [ 0.  ,  0.01,  0.  ,  0.  ,  0.  ],\n",
       "       [-0.  ,  0.  ,  0.01, -0.  ,  0.  ],\n",
       "       [-0.  ,  0.  , -0.  ,  0.01, -0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , -0.  ,  0.01]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data_covar, axis=0).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BROKEN: Kronecker product version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def comm_mat(m, n):\n",
    "    # determine permutation applied by K\n",
    "    w = np.arange(m * n).reshape((m, n), order=\"F\").T.ravel(order=\"F\")\n",
    "    # apply this permutation to the rows (i.e. to each column) of identity matrix and return result\n",
    "    return np.eye(m * n)[w, :]\n",
    "\n",
    "def vec(X):\n",
    "    return np.ravel(X, order='F')\n",
    "\n",
    "def kron_Var_invwish(psi, dof):\n",
    "    p = len(psi)\n",
    "    c2 = ((dof - p)*(dof - p - 1)*(dof - p - 3))**(-1)\n",
    "    c1 = (dof-p-2)*c2\n",
    "    c3 = (dof-p-1)**(-2)\n",
    "    K_pp = comm_mat(p, p)\n",
    "    return c1 * np.kron(psi, psi) + c2*vec(psi) @ vec(psi).T + c2 * K_pp @ np.kron(psi, psi) \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a3043ce43b8894bffef93d1839881ad472cb7607c1a3c9f1cfc63c042591dd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
