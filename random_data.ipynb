{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7156eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import multivariate_normal, invwishart\n",
    "def random_data_gen(n_samples=1000, n_feats=10, maha=1.0, psi_diag=1.0, psi_offdiag=0., ddof=4, class_ratio=0.5, seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    ## initialize multivariate normal dist with normally distributed means and covariance\n",
    "    ## drawn from an inverse wishart distribution (conjugate prior for MVN)\n",
    "    norm_means_a = np.random.randn(n_feats)\n",
    "    norm_means_b = np.zeros_like(norm_means_a)\n",
    "    psi = psi_diag * np.eye(n_feats) + psi_offdiag * ~np.eye(n_feats).astype(bool)\n",
    "    nu = n_feats + ddof\n",
    "    wishart_cov = invwishart(nu, psi).rvs()\n",
    "    ## specify the mahalanobis distance between the two distributions\n",
    "    dist = mahalanobis(norm_means_a, norm_means_b, wishart_cov)\n",
    "    norm_means_a = norm_means_a * (maha / dist)\n",
    "    assert np.isclose(mahalanobis(norm_means_a, norm_means_b, wishart_cov), maha)\n",
    "    ## multivariate normal distributions with different means and equal variances\n",
    "    mvn_a = multivariate_normal(mean=norm_means_a, cov=wishart_cov)\n",
    "    mvn_b = multivariate_normal(mean=norm_means_b, cov=wishart_cov)\n",
    "    ## not used, but compute correlations\n",
    "    corr = (D:=np.diag(1/np.sqrt(np.diag(wishart_cov)))) @ wishart_cov @ D\n",
    "    ## generate data samples from a multivariate normal\n",
    "    data = np.vstack([mvn_a.rvs(int(n_samples*class_ratio)), mvn_b.rvs(n_samples - int(n_samples*class_ratio))])\n",
    "    labels = np.arange(len(data))<int(n_samples*class_ratio)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "0ce73d12",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.90587371,  0.85330968,  0.78026648, ...,  0.88040803,\n",
       "          1.22170708, -0.22122927],\n",
       "        [ 1.38490785,  0.15912619, -0.88120683, ..., -1.12625305,\n",
       "          1.08151514, -1.26325278],\n",
       "        [ 1.1851155 , -0.92126382, -0.70761187, ..., -1.38830525,\n",
       "         -0.00730406, -1.56451073],\n",
       "        ...,\n",
       "        [ 0.03138741, -0.97906704, -0.09509917, ..., -0.04007228,\n",
       "         -0.31558083,  0.35233976],\n",
       "        [-0.29551414,  0.30137921,  0.6470991 , ...,  0.73653539,\n",
       "         -0.25380056,  0.54616535],\n",
       "        [-0.71440345, -0.31405635,  0.08474968, ...,  0.94248388,\n",
       "         -0.80891815, -0.44761239]]),\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False]))"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = random_data_gen(n_samples=1000, n_feats=10, maha=1., psi_diag=1.)\n",
    "data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44869a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"random_data_X.npy\", data)\n",
    "np.save(\"random_data_y\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "3774863c-4cfc-417a-89da-f04dd824f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_invwish(psi, dof):\n",
    "    n_feats = len(psi)\n",
    "    return psi / (dof-n_feats-1)\n",
    "\n",
    "def Var_invwish(psi, dof):\n",
    "    p = len(psi)\n",
    "    Var = np.empty((p, p))\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            Var[i][j] = (dof-p+1) * psi[i][j]**2 + (dof-p-1) * psi[i][i]*psi[j][j] \n",
    "    Var /= (dof-p)*(dof-p-1)**2*(dof-p-3)\n",
    "    return Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a467f5b0-cd89-464f-8b5a-9d186452c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance_invwishart(diag=1., offdiag=0., ddof=4, p=5):\n",
    "    psi = diag*np.eye(5) + offdiag * np.ones((p, p))\n",
    "    expected = E_invwish(psi, p+ddof)\n",
    "    variance = Var_invwish(psi, p+ddof)\n",
    "    return expected, variance \n",
    "\n",
    "expected, variance = covariance_invwishart(2, 1)\n",
    "print(f\"Expected Cov:\\n{expected}\")\n",
    "corr = (D:=np.diag(1/np.sqrt(np.diag(expected)))) @ expected @ D\n",
    "print(f\"Expected Corr:\\n{corr}\")\n",
    "print(f\"Variance of Cov:\\n{variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "a9dd3078-773b-4a06-ad92-662a87941486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5516999 , 0.65217429, 0.65532596, 0.62977027, 0.60524033],\n",
       "       [0.65217429, 1.61624763, 0.64239274, 0.65072092, 0.60476947],\n",
       "       [0.65532596, 0.64239274, 1.76157798, 0.91743822, 0.63559231],\n",
       "       [0.62977027, 0.65072092, 0.91743822, 2.49424013, 0.7386889 ],\n",
       "       [0.60524033, 0.60476947, 0.63559231, 0.7386889 , 1.68301676]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iw_dist = invwishart(df=9, scale=psi)\n",
    "iw_dist.rvs(100000).var(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "99f803d9-e70d-4885-9253-9b85ecc513ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = random_data_gen(n_samples=1000, n_feats=10, maha=1., scale_diag=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "d9ad0600-20e3-48ce-8b45-6f867aa891bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_covar= np.stack([np.cov(random_data_gen(n_samples=200, n_feats=5, maha=1., psi_diag=3.)[0][:100].T) for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "55ed53bc-eb86-4ae8-80e3-f9ada5e55c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95,  0.03, -0.01,  0.  , -0.07],\n",
       "       [ 0.03,  0.99, -0.02, -0.01, -0.01],\n",
       "       [-0.01, -0.02,  0.97,  0.03,  0.01],\n",
       "       [ 0.  , -0.01,  0.03,  0.95, -0.  ],\n",
       "       [-0.07, -0.01,  0.01, -0.  ,  0.99]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data_covar, axis=0).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5596337-ae69-411e-8173-5dec5ddd3d95",
   "metadata": {},
   "source": [
    "### Kronecker product version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c4bd53eb-0087-47ca-b72a-8191438e9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def comm_mat(m, n):\n",
    "    # determine permutation applied by K\n",
    "    w = np.arange(m * n).reshape((m, n), order=\"F\").T.ravel(order=\"F\")\n",
    "    # apply this permutation to the rows (i.e. to each column) of identity matrix and return result\n",
    "    return np.eye(m * n)[w, :]\n",
    "\n",
    "def vec(X):\n",
    "    return np.ravel(X, order='F')\n",
    "\n",
    "def kron_Var_invwish(psi, dof):\n",
    "    p = len(psi)\n",
    "    c2 = ((dof - p)*(dof - p - 1)*(dof - p - 3))**(-1)\n",
    "    c1 = (dof-p-2)*c2\n",
    "    c3 = (dof-p-1)**(-2)\n",
    "    K_pp = comm_mat(p, p)\n",
    "    return c1 * np.kron(psi, psi) + c2*vec(psi) @ vec(psi).T + c2 * K_pp @ np.kron(psi, psi) \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a3043ce43b8894bffef93d1839881ad472cb7607c1a3c9f1cfc63c042591dd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
