{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import multivariate_normal, invwishart\n",
    "from permutation_helpers import random_data_gen\n",
    "# def random_data_gen(n_samples=1000, n_feats=10, maha=1.0, psi_diag=1.0, psi_offdiag=0., ddof=150, class_ratio=0.5, seed=None):\n",
    "#     if seed:\n",
    "#         np.random.seed(seed)\n",
    "#     ## initialize multivariate normal dist with normally distributed means and covariance\n",
    "#     ## drawn from an inverse wishart distribution (conjugate prior for MVN)\n",
    "#     norm_means_a = np.random.randn(n_feats)\n",
    "#     norm_means_b = np.zeros_like(norm_means_a)\n",
    "#     psi = psi_diag * np.eye(n_feats) + psi_offdiag * ~np.eye(n_feats).astype(bool)\n",
    "#     nu = n_feats + ddof\n",
    "#     wishart_cov = invwishart(nu, psi).rvs()\n",
    "#     ## specify the mahalanobis distance between the two distributions\n",
    "#     dist = mahalanobis(norm_means_a, norm_means_b, np.linalg.inv(wishart_cov))\n",
    "#     norm_means_a = norm_means_a * (maha / dist)\n",
    "#     assert np.isclose(mahalanobis(norm_means_a, norm_means_b, np.linalg.inv(wishart_cov)), maha)\n",
    "#     ## multivariate normal distributions with different means and equal variances\n",
    "#     mvn_a = multivariate_normal(mean=norm_means_a, cov=wishart_cov)\n",
    "#     mvn_b = multivariate_normal(mean=norm_means_b, cov=wishart_cov)\n",
    "#     ## not used, but compute correlations\n",
    "#     corr = (D:=np.diag(1/np.sqrt(np.diag(wishart_cov)))) @ wishart_cov @ D\n",
    "#     ## generate data samples from a multivariate normal\n",
    "#     data = np.vstack([mvn_a.rvs(int(n_samples*class_ratio)), mvn_b.rvs(n_samples - int(n_samples*class_ratio))])\n",
    "#     labels = np.arange(len(data))<int(n_samples*class_ratio)\n",
    "#     return data, labels\n",
    "# #     idx = np.random.choice(np.arange(n_samples), n_samples, replace=False)\n",
    "# #     return data[idx], labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.05388803, -0.04537587, -0.12044986, ..., -0.1208464 ,\n",
       "          0.0104293 ,  0.01338025],\n",
       "        [ 0.04694033,  0.01004464, -0.01491472, ..., -0.05995696,\n",
       "          0.02228193, -0.0760042 ],\n",
       "        [-0.07455321,  0.05215185,  0.00418394, ..., -0.0290755 ,\n",
       "         -0.03820365,  0.00456696],\n",
       "        ...,\n",
       "        [ 0.01424548,  0.09464702, -0.11352255, ...,  0.08822633,\n",
       "          0.08589731, -0.06343076],\n",
       "        [-0.00605004,  0.0354669 ,  0.09935671, ...,  0.03441683,\n",
       "          0.08805234,  0.08118872],\n",
       "        [ 0.03475752,  0.13853059, -0.08067336, ..., -0.1465611 ,\n",
       "         -0.06162906, -0.0460488 ]]),\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = random_data_gen(n_samples=1000, n_feats=10, maha=1., psi_diag=1., seed=1)\n",
    "data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"random_data_X.npy\", data)\n",
    "np.save(\"random_data_y\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_invwish(psi, dof):\n",
    "    n_feats = len(psi)\n",
    "    return psi / (dof-n_feats-1)\n",
    "\n",
    "def Var_invwish(psi, dof):\n",
    "    p = len(psi)\n",
    "    Var = np.empty((p, p))\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            Var[i][j] = (dof-p+1) * psi[i][j]**2 + (dof-p-1) * psi[i][i]*psi[j][j] \n",
    "    Var /= (dof-p)*(dof-p-1)**2*(dof-p-3)\n",
    "    return Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Cov:\n",
      "[[0.1 0.  0.  0.  0. ]\n",
      " [0.  0.1 0.  0.  0. ]\n",
      " [0.  0.  0.1 0.  0. ]\n",
      " [0.  0.  0.  0.1 0. ]\n",
      " [0.  0.  0.  0.  0.1]]\n",
      "Expected Corr:\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "Variance of Cov:\n",
      "[[0.0025     0.00113636 0.00113636 0.00113636 0.00113636]\n",
      " [0.00113636 0.0025     0.00113636 0.00113636 0.00113636]\n",
      " [0.00113636 0.00113636 0.0025     0.00113636 0.00113636]\n",
      " [0.00113636 0.00113636 0.00113636 0.0025     0.00113636]\n",
      " [0.00113636 0.00113636 0.00113636 0.00113636 0.0025    ]]\n"
     ]
    }
   ],
   "source": [
    "def covariance_invwishart(diag=1., offdiag=0., ddof=4, p=5):\n",
    "    psi = diag*np.eye(5) + offdiag * np.ones((p, p))\n",
    "    expected = E_invwish(psi, p+ddof)\n",
    "    variance = Var_invwish(psi, p+ddof)\n",
    "    return expected, variance \n",
    "\n",
    "expected, variance = covariance_invwishart(1, 0, ddof=11)\n",
    "print(f\"Expected Cov:\\n{expected}\")\n",
    "corr = (D:=np.diag(1/np.sqrt(np.diag(expected)))) @ expected @ D\n",
    "print(f\"Expected Corr:\\n{corr}\")\n",
    "print(f\"Variance of Cov:\\n{variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = random_data_gen(n_samples=1000, n_feats=10, maha=1., psi_diag=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.2007182 ,  -0.95580753,  -0.07464831,   0.79587807,\n",
       "         -0.11274548,   1.70641732,   0.9288462 ,  -1.32456017,\n",
       "         -0.83025096,  -0.77377264],\n",
       "       [ -0.95580753,   4.7273892 ,   0.36158669,  -3.92550811,\n",
       "          0.55842923,  -8.42368657,  -4.5875671 ,   6.53652991,\n",
       "          4.09937494,   3.8183708 ],\n",
       "       [ -0.07464831,   0.36158669,   0.0330118 ,  -0.30172812,\n",
       "          0.04204758,  -0.64561534,  -0.35099156,   0.50061088,\n",
       "          0.31418397,   0.29304502],\n",
       "       [  0.79587807,  -3.92550811,  -0.30172812,   3.27417942,\n",
       "         -0.46415232,   7.0063356 ,   3.81405604,  -5.43539216,\n",
       "         -3.40979621,  -3.17632156],\n",
       "       [ -0.11274548,   0.55842923,   0.04204758,  -0.46415232,\n",
       "          0.07216928,  -0.99535321,  -0.54296266,   0.7729501 ,\n",
       "          0.48457784,   0.45072985],\n",
       "       [  1.70641732,  -8.42368657,  -0.64561534,   7.0063356 ,\n",
       "         -0.99535321,  15.04228213,   8.18603897, -11.66605471,\n",
       "         -7.31753209,  -6.8162875 ],\n",
       "       [  0.9288462 ,  -4.5875671 ,  -0.35099156,   3.81405604,\n",
       "         -0.54296266,   8.18603897,   4.46396405,  -6.3530301 ,\n",
       "         -3.98428568,  -3.70998691],\n",
       "       [ -1.32456017,   6.53652991,   0.50061088,  -5.43539216,\n",
       "          0.7729501 , -11.66605471,  -6.3530301 ,   9.05849914,\n",
       "          5.6780866 ,   5.28812357],\n",
       "       [ -0.83025096,   4.09937494,   0.31418397,  -3.40979621,\n",
       "          0.48457784,  -7.31753209,  -3.98428568,   5.6780866 ,\n",
       "          3.56896295,   3.31645887],\n",
       "       [ -0.77377264,   3.8183708 ,   0.29304502,  -3.17632156,\n",
       "          0.45072985,  -6.8162875 ,  -3.70998691,   5.28812357,\n",
       "          3.31645887,   3.09543957]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_covar= np.stack([np.cov(random_data_gen(n_samples=200, n_feats=10, maha=1., psi_diag=1., ddof=11)[0][:100].T) for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corr = np.stack([(D:=np.diag(1/np.sqrt(np.diag(wishart_cov)))) @ wishart_cov @ D for wishart_cov in data_covar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.        , -0.55795858, -0.60883996, -0.58746159,\n",
       "         -0.51218008, -0.57995721, -0.58177275, -0.57232002,\n",
       "         -0.56681246, -0.5560712 ],\n",
       "        [-0.55795858,  1.        , -0.57296707, -0.56110184,\n",
       "         -0.53871819, -0.56130545, -0.55770361, -0.53110691,\n",
       "         -0.55244428, -0.58464229],\n",
       "        [-0.60883996, -0.57296707,  1.        , -0.53646672,\n",
       "         -0.51398632, -0.57659415, -0.55571139, -0.55046181,\n",
       "         -0.59007017, -0.56264305],\n",
       "        [-0.58746159, -0.56110184, -0.53646672,  1.        ,\n",
       "         -0.54270057, -0.5316269 , -0.54708017, -0.54963091,\n",
       "         -0.60527435, -0.55330917],\n",
       "        [-0.51218008, -0.53871819, -0.51398632, -0.54270057,\n",
       "          1.        , -0.54404358, -0.5516333 , -0.57946057,\n",
       "         -0.53218075, -0.56702601],\n",
       "        [-0.57995721, -0.56130545, -0.57659415, -0.5316269 ,\n",
       "         -0.54404358,  1.        , -0.53555848, -0.54211642,\n",
       "         -0.54651206, -0.57432421],\n",
       "        [-0.58177275, -0.55770361, -0.55571139, -0.54708017,\n",
       "         -0.5516333 , -0.53555848,  1.        , -0.56118271,\n",
       "         -0.5377014 , -0.54834139],\n",
       "        [-0.57232002, -0.53110691, -0.55046181, -0.54963091,\n",
       "         -0.57946057, -0.54211642, -0.56118271,  1.        ,\n",
       "         -0.55618589, -0.5938343 ],\n",
       "        [-0.56681246, -0.55244428, -0.59007017, -0.60527435,\n",
       "         -0.53218075, -0.54651206, -0.5377014 , -0.55618589,\n",
       "          1.        , -0.58000572],\n",
       "        [-0.5560712 , -0.58464229, -0.56264305, -0.55330917,\n",
       "         -0.56702601, -0.57432421, -0.54834139, -0.5938343 ,\n",
       "         -0.58000572,  1.        ]],\n",
       "\n",
       "       [[ 1.        ,  0.54542421,  0.56317067,  0.57542273,\n",
       "          0.5822259 ,  0.56918349,  0.56539059,  0.58453935,\n",
       "          0.55729531,  0.56250899],\n",
       "        [ 0.54542421,  1.        ,  0.53056352,  0.59434045,\n",
       "          0.55517698,  0.55923535,  0.55617092,  0.55302759,\n",
       "          0.53256205,  0.56270224],\n",
       "        [ 0.56317067,  0.53056352,  1.        ,  0.56440089,\n",
       "          0.54985498,  0.56795507,  0.54764962,  0.57564189,\n",
       "          0.538368  ,  0.54937777],\n",
       "        [ 0.57542273,  0.59434045,  0.56440089,  1.        ,\n",
       "          0.54056169,  0.52632998,  0.55169325,  0.54582431,\n",
       "          0.53014787,  0.57256909],\n",
       "        [ 0.5822259 ,  0.55517698,  0.54985498,  0.54056169,\n",
       "          1.        ,  0.56401668,  0.5548158 ,  0.55208205,\n",
       "          0.55692798,  0.57254878],\n",
       "        [ 0.56918349,  0.55923535,  0.56795507,  0.52632998,\n",
       "          0.56401668,  1.        ,  0.53029672,  0.53755486,\n",
       "          0.54717694,  0.53830155],\n",
       "        [ 0.56539059,  0.55617092,  0.54764962,  0.55169325,\n",
       "          0.5548158 ,  0.53029672,  1.        ,  0.5662279 ,\n",
       "          0.54784104,  0.53246213],\n",
       "        [ 0.58453935,  0.55302759,  0.57564189,  0.54582431,\n",
       "          0.55208205,  0.53755486,  0.5662279 ,  1.        ,\n",
       "          0.56217732,  0.54987542],\n",
       "        [ 0.55729531,  0.53256205,  0.538368  ,  0.53014787,\n",
       "          0.55692798,  0.54717694,  0.54784104,  0.56217732,\n",
       "          1.        ,  0.55758628],\n",
       "        [ 0.56250899,  0.56270224,  0.54937777,  0.57256909,\n",
       "          0.57254878,  0.53830155,  0.53246213,  0.54987542,\n",
       "          0.55758628,  1.        ]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(data_corr, q=[.025, .975], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.        , -0.38559518, -0.38700025, -0.37589341,\n",
       "         -0.38991168, -0.3675221 , -0.37777729, -0.37870946,\n",
       "         -0.36564645, -0.38367836],\n",
       "        [-0.38559518,  1.        , -0.39943433, -0.38765437,\n",
       "         -0.37406198, -0.35338062, -0.36536066, -0.3935133 ,\n",
       "         -0.37319157, -0.3811974 ],\n",
       "        [-0.38700025, -0.39943433,  1.        , -0.41049365,\n",
       "         -0.39463701, -0.38492749, -0.39160884, -0.38795728,\n",
       "         -0.38556426, -0.37863835],\n",
       "        [-0.37589341, -0.38765437, -0.41049365,  1.        ,\n",
       "         -0.38770109, -0.37313139, -0.37616257, -0.41345547,\n",
       "         -0.38100565, -0.395552  ],\n",
       "        [-0.38991168, -0.37406198, -0.39463701, -0.38770109,\n",
       "          1.        , -0.38165204, -0.39418552, -0.37618871,\n",
       "         -0.37721229, -0.40002847],\n",
       "        [-0.3675221 , -0.35338062, -0.38492749, -0.37313139,\n",
       "         -0.38165204,  1.        , -0.37754554, -0.39649449,\n",
       "         -0.34929716, -0.38527781],\n",
       "        [-0.37777729, -0.36536066, -0.39160884, -0.37616257,\n",
       "         -0.39418552, -0.37754554,  1.        , -0.37377921,\n",
       "         -0.37483615, -0.39870899],\n",
       "        [-0.37870946, -0.3935133 , -0.38795728, -0.41345547,\n",
       "         -0.37618871, -0.39649449, -0.37377921,  1.        ,\n",
       "         -0.39281188, -0.40174932],\n",
       "        [-0.36564645, -0.37319157, -0.38556426, -0.38100565,\n",
       "         -0.37721229, -0.34929716, -0.37483615, -0.39281188,\n",
       "          1.        , -0.37380225],\n",
       "        [-0.38367836, -0.3811974 , -0.37863835, -0.395552  ,\n",
       "         -0.40002847, -0.38527781, -0.39870899, -0.40174932,\n",
       "         -0.37380225,  1.        ]],\n",
       "\n",
       "       [[ 1.        ,  0.38420708,  0.37093674,  0.3825018 ,\n",
       "          0.38669297,  0.39425235,  0.34509915,  0.39846959,\n",
       "          0.38033972,  0.38166872],\n",
       "        [ 0.38420708,  1.        ,  0.38234012,  0.4063877 ,\n",
       "          0.39549268,  0.38604442,  0.40674387,  0.39266133,\n",
       "          0.35152567,  0.40590646],\n",
       "        [ 0.37093674,  0.38234012,  1.        ,  0.36459989,\n",
       "          0.39418119,  0.36794394,  0.40451839,  0.35973225,\n",
       "          0.35724721,  0.39025123],\n",
       "        [ 0.3825018 ,  0.4063877 ,  0.36459989,  1.        ,\n",
       "          0.37225688,  0.40254786,  0.38497753,  0.39773714,\n",
       "          0.3943761 ,  0.37736504],\n",
       "        [ 0.38669297,  0.39549268,  0.39418119,  0.37225688,\n",
       "          1.        ,  0.36583157,  0.37423126,  0.37456532,\n",
       "          0.39785179,  0.38427822],\n",
       "        [ 0.39425235,  0.38604442,  0.36794394,  0.40254786,\n",
       "          0.36583157,  1.        ,  0.41096432,  0.38811516,\n",
       "          0.37054534,  0.40658096],\n",
       "        [ 0.34509915,  0.40674387,  0.40451839,  0.38497753,\n",
       "          0.37423126,  0.41096432,  1.        ,  0.41712234,\n",
       "          0.37343456,  0.38299879],\n",
       "        [ 0.39846959,  0.39266133,  0.35973225,  0.39773714,\n",
       "          0.37456532,  0.38811516,  0.41712234,  1.        ,\n",
       "          0.37635428,  0.38751419],\n",
       "        [ 0.38033972,  0.35152567,  0.35724721,  0.3943761 ,\n",
       "          0.39785179,  0.37054534,  0.37343456,  0.37635428,\n",
       "          1.        ,  0.39872017],\n",
       "        [ 0.38166872,  0.40590646,  0.39025123,  0.37736504,\n",
       "          0.38427822,  0.40658096,  0.38299879,  0.38751419,\n",
       "          0.39872017,  1.        ]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(data_corr, q=[.1, .9], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BROKEN: Kronecker product version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def comm_mat(m, n):\n",
    "    # determine permutation applied by K\n",
    "    w = np.arange(m * n).reshape((m, n), order=\"F\").T.ravel(order=\"F\")\n",
    "    # apply this permutation to the rows (i.e. to each column) of identity matrix and return result\n",
    "    return np.eye(m * n)[w, :]\n",
    "\n",
    "def vec(X):\n",
    "    return np.ravel(X, order='F')\n",
    "\n",
    "def kron_Var_invwish(psi, dof):\n",
    "    p = len(psi)\n",
    "    c2 = ((dof - p)*(dof - p - 1)*(dof - p - 3))**(-1)\n",
    "    c1 = (dof-p-2)*c2\n",
    "    c3 = (dof-p-1)**(-2)\n",
    "    K_pp = comm_mat(p, p)\n",
    "    return c1 * np.kron(psi, psi) + c2*vec(psi) @ vec(psi).T + c2 * K_pp @ np.kron(psi, psi) \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "permutation-tests-aQMIHBsu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
