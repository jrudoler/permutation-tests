{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def log_svm(Input_matrix, labels, k_folds=5, lda_flag=False, normalize=False):\n",
    "    \"\"\"\n",
    "    Translates the given MATLAB SVM function to Python using scikit-learn.\n",
    "    Performs random K-fold cross-validation without stratification.\n",
    "\n",
    "    Parameters:\n",
    "    - Input_matrix: Features (X) of the dataset\n",
    "    - labels: True labels (y) of the dataset\n",
    "    - k_folds: Number of folds for cross-validation (default: 5)\n",
    "    - lda_flag: Flag to apply LDA or not (default: False)\n",
    "    - normalize: Flag to normalize input data (default: False)\n",
    "\n",
    "    Returns:\n",
    "    - hit_rate: Overall accuracy (hit rate)\n",
    "    - confusion_matrix_normalized: Normalized confusion matrix\n",
    "    - errors_per_fold: List of misclassifications per fold\n",
    "    \"\"\"\n",
    "\n",
    "    input_matrix = np.array(Input_matrix, dtype=float)\n",
    "    labels = np.array(labels, dtype=float)\n",
    "\n",
    "    # Normalize data if required\n",
    "    if normalize:\n",
    "        scaler = StandardScaler()\n",
    "        input_matrix = scaler.fit_transform(input_matrix)\n",
    "\n",
    "    total_trials = input_matrix.shape[0]\n",
    "    predicted_labels = np.zeros(labels.shape)\n",
    "    cumulative_error = 0\n",
    "    errors_per_fold = []\n",
    "\n",
    "    # Using KFold for random cross-validation (not stratified)\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in kfold.split(input_matrix):\n",
    "        train_data, test_data = input_matrix[train_index], input_matrix[test_index]\n",
    "        train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "\n",
    "        # Apply LDA if lda_flag is set (scikit-learn's LDA)\n",
    "        if lda_flag:\n",
    "            from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "            lda = LinearDiscriminantAnalysis()\n",
    "            train_data = lda.fit_transform(train_data, train_labels)\n",
    "            test_data = lda.transform(test_data)\n",
    "\n",
    "        # Train SVM model (linear kernel: kernel='linear', RBF kernel: kernel='rbf')\n",
    "        classifier = svm.SVC(\n",
    "            kernel=\"linear\"\n",
    "        )  # For linear kernel, set kernel='rbf' for RBF kernel\n",
    "        classifier.fit(train_data, train_labels)\n",
    "        predicted_test_labels = classifier.predict(test_data)\n",
    "\n",
    "        # Count misclassifications\n",
    "        fold_error = np.sum(test_labels != predicted_test_labels)\n",
    "        errors_per_fold.append(fold_error)\n",
    "        cumulative_error += fold_error\n",
    "        predicted_labels[test_index] = predicted_test_labels\n",
    "\n",
    "    # Compute performance of the decoder\n",
    "    hit_rate = 1 - (cumulative_error / total_trials)\n",
    "\n",
    "    # Compute confusion matrix and normalize it\n",
    "    confusion_matrix_result = confusion_matrix(labels, predicted_labels)\n",
    "    confusion_matrix_normalized = (\n",
    "        confusion_matrix_result.astype(float)\n",
    "        / confusion_matrix_result.sum(axis=1)[:, np.newaxis]\n",
    "    )\n",
    "\n",
    "    return hit_rate, confusion_matrix_normalized, errors_per_fold\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# input_matrix = ...  # Your data\n",
    "# labels = ...        # Your labels\n",
    "# hit_rate, conf_matrix, errors = log_svm(input_matrix, labels, k_folds=5, lda_flag=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATLAB 5.0 MAT-file, Platform: MACA64, Created on: Mon Oct 28 14:24:41 2024                                         \u001a\u0001\u0000\u0000\u0000\u0000\u0000\u0001IM\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# filepath = 'your_file.mat'  # Replace with your actual file path\n",
    "\n",
    "with open(filepath, 'rb') as f:\n",
    "    header = f.read(128)  # The MATLAB file header is 128 bytes\n",
    "    try:\n",
    "        header_text = header.decode('utf-8', errors='ignore')\n",
    "    except UnicodeDecodeError:\n",
    "        header_text = header.decode('latin1', errors='ignore')\n",
    "    print(header_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (8812, 1, 1) instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m behavior_labels \u001b[38;5;241m=\u001b[39m behavior_labels[idx_single]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Compute frequency of behavior for the session\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m behav_freq_table \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbehavior_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m     95\u001b[0m behav_freq_table \u001b[38;5;241m=\u001b[39m behav_freq_table[behav_freq_table \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m]  \u001b[38;5;66;03m# Minimum occurrences\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Select behaviors with a minimum number of occurrences\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/permutation-tests-USYHTt1K-py3.12/lib/python3.12/site-packages/pandas/core/series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/permutation-tests-USYHTt1K-py3.12/lib/python3.12/site-packages/pandas/core/construction.py:659\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    656\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n\u001b[1;32m    657\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m maybe_infer_to_datetimelike(subarr)\n\u001b[0;32m--> 659\u001b[0m subarr \u001b[38;5;241m=\u001b[39m \u001b[43m_sanitize_ndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[1;32m    663\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mdtype, dtype)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/permutation-tests-USYHTt1K-py3.12/lib/python3.12/site-packages/pandas/core/construction.py:718\u001b[0m, in \u001b[0;36m_sanitize_ndim\u001b[0;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allow_2d:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must be 1-dimensional, got ndarray of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    720\u001b[0m     )\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     result \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: Data must be 1-dimensional, got ndarray of shape (8812, 1, 1) instead"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Function to load data\n",
    "def load_data(file_path):\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    return (\n",
    "        mat_data[\"Spike_rasters\"],\n",
    "        mat_data[\"labels\"],\n",
    "        mat_data[\"behav_categ\"],\n",
    "        mat_data[\"block_times\"],\n",
    "        mat_data[\"monkey\"],\n",
    "        mat_data[\"unit_count\"],\n",
    "        mat_data[\"groom_labels_all\"],\n",
    "        mat_data[\"brain_label\"],\n",
    "        mat_data[\"behavior_log\"],\n",
    "        mat_data[\"behav_categ_original\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "is_mac = True\n",
    "home = os.path.expanduser(\"~\") if is_mac else \"C:/Users/GENERAL\"\n",
    "source_directory = os.path.join(home, \"Dropbox (Penn)/data/monkey\")\n",
    "a_directory = os.path.join(source_directory, \"Data_MonkeyA\")\n",
    "h_directory = os.path.join(source_directory, \"Data_MonkeyH\")\n",
    "\n",
    "# Get all session directories\n",
    "a_sessions = [\n",
    "    os.path.join(a_directory, f)\n",
    "    for f in os.listdir(a_directory)\n",
    "    if not f.startswith(\".\")\n",
    "]\n",
    "h_sessions = [\n",
    "    os.path.join(h_directory, f)\n",
    "    for f in os.listdir(h_directory)\n",
    "    if not f.startswith(\".\")\n",
    "]\n",
    "all_sessions = a_sessions + h_sessions  # Combine both session lists\n",
    "# sessions to skip\n",
    "sessions_to_skip = [\"Hooke_2021-09-09\", \"Amos_2021-09-14\"]  # missing necessary files, or other\n",
    "all_sessions = [session for session in all_sessions if not any(skip in session for skip in sessions_to_skip)]\n",
    "\n",
    "\n",
    "mean_hitrate = []\n",
    "sd_hitrate = []\n",
    "mean_hitrate_shuffled = []\n",
    "\n",
    "for session_path in all_sessions:\n",
    "    # Check if the session path is valid and contains the necessary files\n",
    "    if not os.path.exists(session_path):\n",
    "        print(f\"Session path does not exist: {session_path}\")\n",
    "        continue\n",
    "\n",
    "    for channel_flag in [\"vlPFC\", \"TEO\", \"all\"]:\n",
    "        filepath = os.path.join(session_path, f'processed_for_SVM_{channel_flag}.mat')\n",
    "        \n",
    "        try:\n",
    "            mat_data = sio.loadmat(filepath)\n",
    "            \n",
    "            # Assign the output to appropriate Pythonic variables\n",
    "            spike_rasters, labels, labels_partner, behav_categ, block_times, monkey, unit_count, groom_labels_all, brain_label, behavior_log, behav_categ_original = (\n",
    "                mat_data[\"Spike_rasters\"],\n",
    "                mat_data[\"labels\"],\n",
    "                mat_data[\"labels_partner\"],\n",
    "                mat_data[\"behav_categ\"],\n",
    "                mat_data[\"block_times\"],\n",
    "                mat_data[\"monkey\"],\n",
    "                mat_data[\"unit_count\"],\n",
    "                mat_data[\"groom_labels_all\"],\n",
    "                mat_data[\"brain_label\"],\n",
    "                mat_data[\"behavior_log\"],\n",
    "                mat_data[\"behav_categ_original\"]\n",
    "            )\n",
    "            \n",
    "            # Process data\n",
    "            spike_count_raster = spike_rasters.T\n",
    "            behavior_labels = np.array([label[2] for label in labels])  # Extract unique behavior info\n",
    "            co_occurrence = np.array([label[4] for label in labels])\n",
    "            \n",
    "            # Select epochs where only one behavior happens\n",
    "            idx_single = np.where(co_occurrence < 4)[0]\n",
    "            spike_count_raster = spike_count_raster[idx_single, :]\n",
    "            behavior_labels = behavior_labels[idx_single]\n",
    "            \n",
    "            # Compute frequency of behavior for the session\n",
    "            behav_freq_table = pd.Series(behavior_labels).value_counts()\n",
    "            behav_freq_table = behav_freq_table[behav_freq_table >= 30]  # Minimum occurrences\n",
    "            \n",
    "            # Select behaviors with a minimum number of occurrences\n",
    "            behav = behav_freq_table.index\n",
    "            \n",
    "            # Only keep the behaviors of interest\n",
    "            idx = np.isin(behavior_labels, behav)\n",
    "            spike_count_raster_final = spike_count_raster[idx, :]\n",
    "            behavior_labels_final = behavior_labels[idx]\n",
    "            \n",
    "            # Run SVM using the log_svm function\n",
    "            hit_rate, conf_matrix, errors = log_svm(\n",
    "                spike_count_raster_final,\n",
    "                behavior_labels_final,\n",
    "                k_folds=5,\n",
    "                lda_flag=False,\n",
    "                normalize=True,\n",
    "            )\n",
    "            \n",
    "            mean_hitrate.append(hit_rate)\n",
    "            sd_hitrate.append(np.std(errors))  # Assuming errors is a list of misclassifications per fold\n",
    "            mean_hitrate_shuffled.append(np.mean(errors))  # Adjust as needed\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}. Continuing without this data.\")\n",
    "            continue\n",
    "\n",
    "# Plotting results\n",
    "plt.figure()\n",
    "plt.bar(\n",
    "    [\"Real\", \"Shuffled\"],\n",
    "    [np.mean(mean_hitrate), np.mean(mean_hitrate_shuffled)],\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.ylabel(\"Decoding Accuracy\")\n",
    "plt.title(\"Decoding accuracy for subject current behavioral states\")\n",
    "# plt.savefig(\"SVM_results_subjectBehav.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from utils import load_data, preprocess_data\n",
    "# from macaques_example import log_svm  # Assuming log_svm is defined in macaques-example.ipynb and exported\n",
    "\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a .mat file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the .mat file.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing various data arrays.\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    return (\n",
    "        mat_data[\"Spike_rasters\"],\n",
    "        mat_data[\"labels\"],\n",
    "        mat_data.get(\"labels_partner\", None),\n",
    "        mat_data[\"behav_categ\"],\n",
    "        mat_data[\"block_times\"],\n",
    "        mat_data[\"monkey\"],\n",
    "        mat_data[\"unit_count\"],\n",
    "        mat_data[\"groom_labels_all\"],\n",
    "        mat_data[\"brain_label\"],\n",
    "        mat_data[\"behavior_log\"],\n",
    "        mat_data[\"behav_categ_original\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_data(spike_rasters, labels, behav_categ, co_occurrence, min_occurrences=30):\n",
    "    \"\"\"\n",
    "    Preprocess spike and behavior data.\n",
    "\n",
    "    Parameters:\n",
    "    - spike_rasters: Spike raster data.\n",
    "    - labels: Behavior labels.\n",
    "    - behav_categ: Behavior categories.\n",
    "    - co_occurrence: Co-occurrence data.\n",
    "    - min_occurrences: Minimum number of occurrences to consider a behavior.\n",
    "\n",
    "    Returns:\n",
    "    - Filtered spike rasters and behavior labels.\n",
    "    \"\"\"\n",
    "    spike_count_raster = spike_rasters.T\n",
    "    behavior_labels = np.array([label[2] for label in labels]).flatten()\n",
    "    co_occurrence = np.array([label[4] for label in labels]).flatten()\n",
    "\n",
    "    # Select epochs where only one behavior happens\n",
    "    idx_single = np.where(co_occurrence < 4)[0]\n",
    "    spike_count_raster = spike_count_raster[idx_single, :]\n",
    "    behavior_labels = behavior_labels[idx_single]\n",
    "\n",
    "    # Compute frequency of behavior for the session\n",
    "    behav_freq_table = pd.Series(behavior_labels).value_counts()\n",
    "    behav_freq_table = behav_freq_table[behav_freq_table >= min_occurrences]\n",
    "\n",
    "    behav = behav_freq_table.index\n",
    "\n",
    "    # Only keep the behaviors of interest\n",
    "    idx = np.isin(behavior_labels, behav)\n",
    "    spike_count_raster_final = spike_count_raster[idx, :]\n",
    "    behavior_labels_final = behavior_labels[idx]\n",
    "\n",
    "    return spike_count_raster_final, behavior_labels_final, behav\n",
    "\n",
    "\n",
    "def get_all_sessions(source_directory, sessions_to_skip):\n",
    "    \"\"\"\n",
    "    Retrieve all session directories excluding those to skip.\n",
    "    \"\"\"\n",
    "    a_directory = os.path.join(source_directory, \"Data_MonkeyA\")\n",
    "    h_directory = os.path.join(source_directory, \"Data_MonkeyH\")\n",
    "\n",
    "    a_sessions = [\n",
    "        os.path.join(a_directory, f)\n",
    "        for f in os.listdir(a_directory)\n",
    "        if not f.startswith(\".\")\n",
    "    ]\n",
    "    h_sessions = [\n",
    "        os.path.join(h_directory, f)\n",
    "        for f in os.listdir(h_directory)\n",
    "        if not f.startswith(\".\")\n",
    "    ]\n",
    "    all_sessions = a_sessions + h_sessions\n",
    "    all_sessions = [\n",
    "        session for session in all_sessions\n",
    "        if not any(skip in session for skip in sessions_to_skip)\n",
    "    ]\n",
    "    return all_sessions\n",
    "\n",
    "\n",
    "home = os.path.expanduser(\"~\") if is_mac else \"C:/Users/GENERAL\"\n",
    "source_directory = os.path.join(home, \"Dropbox (Penn)\", \"data\", \"monkey\")\n",
    "sessions_to_skip = [\"Hooke_2021-09-09\", \"Amos_2021-09-14\"]\n",
    "\n",
    "all_sessions = get_all_sessions(source_directory, sessions_to_skip)\n",
    "\n",
    "mean_hitrate = []\n",
    "sd_hitrate = []\n",
    "mean_hitrate_shuffled = []\n",
    "\n",
    "for session_path in all_sessions:\n",
    "    if not os.path.exists(session_path):\n",
    "        print(f\"Session path does not exist: {session_path}\")\n",
    "        continue\n",
    "\n",
    "    for channel_flag in [\"vlPFC\", \"TEO\", \"all\"]:\n",
    "        filepath = os.path.join(session_path, f'processed_for_SVM_{channel_flag}.mat')\n",
    "\n",
    "        try:\n",
    "            # Load data\n",
    "            (\n",
    "                spike_rasters,\n",
    "                labels,\n",
    "                labels_partner,\n",
    "                behav_categ,\n",
    "                block_times,\n",
    "                monkey,\n",
    "                unit_count,\n",
    "                groom_labels_all,\n",
    "                brain_label,\n",
    "                behavior_log,\n",
    "                behav_categ_original\n",
    "            ) = load_data(filepath)\n",
    "\n",
    "            # Preprocess data\n",
    "            spike_raster_final, labels_final, behav = preprocess_data(\n",
    "                spike_rasters,\n",
    "                labels,\n",
    "                behav_categ,\n",
    "                co_occurrence=None  # Modify if co_occurrence is available\n",
    "            )\n",
    "\n",
    "            # Run SVM\n",
    "            hit_rate, conf_matrix, errors = log_svm(\n",
    "                spike_raster_final,\n",
    "                labels_final,\n",
    "                k_folds=5,\n",
    "                lda_flag=False,\n",
    "                normalize=True\n",
    "            )\n",
    "\n",
    "            mean_hitrate.append(hit_rate)\n",
    "            sd_hitrate.append(np.std(errors))\n",
    "            mean_hitrate_shuffled.append(np.mean(errors))\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}. Continuing without this data.\")\n",
    "            continue\n",
    "\n",
    "# Plotting results\n",
    "plt.figure()\n",
    "plt.bar(\n",
    "    [\"Real\", \"Shuffled\"],\n",
    "    [np.mean(mean_hitrate), np.mean(mean_hitrate_shuffled)],\n",
    "    alpha=0.5,\n",
    "    color=['blue', 'orange']\n",
    ")\n",
    "plt.ylabel(\"Decoding Accuracy\")\n",
    "plt.title(\"Decoding accuracy for subject current behavioral states\")\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "results_dir = os.path.join(home, \"Documents\", \"projects\", \"Datalogger\", \"Results\", \"SVM_results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "results_path = os.path.join(results_dir, \"SVM_results_subjectBehav.npy\")\n",
    "np.save(results_path, {\n",
    "    \"mean_hitrate\": mean_hitrate,\n",
    "    \"sd_hitrate\": sd_hitrate,\n",
    "    \"mean_hitrate_shuffled\": mean_hitrate_shuffled,\n",
    "    \"behav\": behav\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.path.join(home, \"Documents\", \"projects\", \"Datalogger\", \"Results\", \"SVM_results\")\n",
    "results_path = os.path.join(results_dir, \"SVM_results_subjectBehav.npy\")\n",
    "results = np.load(results_path, allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91310345, 0.8637931 , 0.95172414],\n",
       "       [0.88685085, 0.85498108, 0.92158661],\n",
       "       [0.85844419, 0.83111625, 0.90124349],\n",
       "       [0.88248272, 0.86071481, 0.92498897],\n",
       "       [0.84613385, 0.80740741, 0.89291748],\n",
       "       [0.88724751, 0.87684655, 0.90774797],\n",
       "       [0.90222541, 0.91213209, 0.93955492],\n",
       "       [0.90513502, 0.90303624, 0.93759619],\n",
       "       [0.87870696, 0.85874912, 0.91384399],\n",
       "       [0.90771106, 0.90084201, 0.94615555],\n",
       "       [0.92986075, 0.91903043, 0.95822589],\n",
       "       [0.91298084, 0.91586227, 0.95389713]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(results['mean_hitrate']).reshape(12, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "permutation-tests-USYHTt1K-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
