{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import multivariate_normal, invwishart\n",
    "def random_data_gen(n_samples=1000, n_feats=10, maha=1.0, psi_diag=1.0, psi_offdiag=0., ddof=150, class_ratio=0.5, seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    ## initialize multivariate normal dist with normally distributed means and covariance\n",
    "    ## drawn from an inverse wishart distribution (conjugate prior for MVN)\n",
    "    norm_means_a = np.random.randn(n_feats)\n",
    "    norm_means_b = np.zeros_like(norm_means_a)\n",
    "    psi = psi_diag * np.eye(n_feats) + psi_offdiag * ~np.eye(n_feats).astype(bool)\n",
    "    nu = n_feats + ddof\n",
    "    wishart_cov = invwishart(nu, psi).rvs()\n",
    "    ## specify the mahalanobis distance between the two distributions\n",
    "    dist = mahalanobis(norm_means_a, norm_means_b, np.linalg.inv(wishart_cov))\n",
    "    norm_means_a = norm_means_a * (maha / dist)\n",
    "    assert np.isclose(mahalanobis(norm_means_a, norm_means_b, np.linalg.inv(wishart_cov)), maha)\n",
    "    ## multivariate normal distributions with different means and equal variances\n",
    "    mvn_a = multivariate_normal(mean=norm_means_a, cov=wishart_cov)\n",
    "    mvn_b = multivariate_normal(mean=norm_means_b, cov=wishart_cov)\n",
    "    ## not used, but compute correlations\n",
    "    corr = (D:=np.diag(1/np.sqrt(np.diag(wishart_cov)))) @ wishart_cov @ D\n",
    "    ## generate data samples from a multivariate normal\n",
    "    data = np.vstack([mvn_a.rvs(int(n_samples*class_ratio)), mvn_b.rvs(n_samples - int(n_samples*class_ratio))])\n",
    "    labels = np.arange(len(data))<int(n_samples*class_ratio)\n",
    "    return data, labels\n",
    "#     idx = np.random.choice(np.arange(n_samples), n_samples, replace=False)\n",
    "#     return data[idx], labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5.60382192e+00, -2.00184839e+00, -1.76000226e+00, ...,\n",
       "         -2.49541696e+00,  1.07216939e+00, -9.22616432e-01],\n",
       "        [ 5.49165612e+00, -2.05865832e+00, -1.84694233e+00, ...,\n",
       "         -2.55926190e+00,  1.06162122e+00, -8.08179665e-01],\n",
       "        [ 5.63523162e+00, -2.13963632e+00, -1.86029316e+00, ...,\n",
       "         -2.57952799e+00,  1.12648119e+00, -8.66350227e-01],\n",
       "        ...,\n",
       "        [-5.07071616e-02,  3.53242082e-03,  4.07076712e-02, ...,\n",
       "         -8.49180686e-02, -8.76625594e-02,  2.72813781e-02],\n",
       "        [ 3.34065677e-02, -1.09287952e-01, -4.01749074e-02, ...,\n",
       "         -2.99504685e-02, -7.43017354e-02, -3.50626539e-02],\n",
       "        [-1.04111421e-01, -4.46627065e-02,  3.69168349e-03, ...,\n",
       "          1.23519444e-01,  4.22717064e-02, -3.64803443e-03]]),\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = random_data_gen(n_samples=1000, n_feats=10, maha=1., psi_diag=1., seed=1)\n",
    "data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"random_data_X.npy\", data)\n",
    "np.save(\"random_data_y\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_invwish(psi, dof):\n",
    "    n_feats = len(psi)\n",
    "    return psi / (dof-n_feats-1)\n",
    "\n",
    "def Var_invwish(psi, dof):\n",
    "    p = len(psi)\n",
    "    Var = np.empty((p, p))\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            Var[i][j] = (dof-p+1) * psi[i][j]**2 + (dof-p-1) * psi[i][i]*psi[j][j] \n",
    "    Var /= (dof-p)*(dof-p-1)**2*(dof-p-3)\n",
    "    return Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Cov:\n",
      "[[0.1 0.  0.  0.  0. ]\n",
      " [0.  0.1 0.  0.  0. ]\n",
      " [0.  0.  0.1 0.  0. ]\n",
      " [0.  0.  0.  0.1 0. ]\n",
      " [0.  0.  0.  0.  0.1]]\n",
      "Expected Corr:\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "Variance of Cov:\n",
      "[[0.0025     0.00113636 0.00113636 0.00113636 0.00113636]\n",
      " [0.00113636 0.0025     0.00113636 0.00113636 0.00113636]\n",
      " [0.00113636 0.00113636 0.0025     0.00113636 0.00113636]\n",
      " [0.00113636 0.00113636 0.00113636 0.0025     0.00113636]\n",
      " [0.00113636 0.00113636 0.00113636 0.00113636 0.0025    ]]\n"
     ]
    }
   ],
   "source": [
    "def covariance_invwishart(diag=1., offdiag=0., ddof=4, p=5):\n",
    "    psi = diag*np.eye(5) + offdiag * np.ones((p, p))\n",
    "    expected = E_invwish(psi, p+ddof)\n",
    "    variance = Var_invwish(psi, p+ddof)\n",
    "    return expected, variance \n",
    "\n",
    "expected, variance = covariance_invwishart(1, 0, ddof=11)\n",
    "print(f\"Expected Cov:\\n{expected}\")\n",
    "corr = (D:=np.diag(1/np.sqrt(np.diag(expected)))) @ expected @ D\n",
    "print(f\"Expected Corr:\\n{corr}\")\n",
    "print(f\"Variance of Cov:\\n{variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'psi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_104269/2320953709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miw_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvwishart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpsi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0miw_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'psi' is not defined"
     ]
    }
   ],
   "source": [
    "iw_dist = invwishart(df=9, scale=psi)\n",
    "iw_dist.rvs(100000).var(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = random_data_gen(n_samples=1000, n_feats=10, maha=1., psi_diag=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.2007182 ,  -0.95580753,  -0.07464831,   0.79587807,\n",
       "         -0.11274548,   1.70641732,   0.9288462 ,  -1.32456017,\n",
       "         -0.83025096,  -0.77377264],\n",
       "       [ -0.95580753,   4.7273892 ,   0.36158669,  -3.92550811,\n",
       "          0.55842923,  -8.42368657,  -4.5875671 ,   6.53652991,\n",
       "          4.09937494,   3.8183708 ],\n",
       "       [ -0.07464831,   0.36158669,   0.0330118 ,  -0.30172812,\n",
       "          0.04204758,  -0.64561534,  -0.35099156,   0.50061088,\n",
       "          0.31418397,   0.29304502],\n",
       "       [  0.79587807,  -3.92550811,  -0.30172812,   3.27417942,\n",
       "         -0.46415232,   7.0063356 ,   3.81405604,  -5.43539216,\n",
       "         -3.40979621,  -3.17632156],\n",
       "       [ -0.11274548,   0.55842923,   0.04204758,  -0.46415232,\n",
       "          0.07216928,  -0.99535321,  -0.54296266,   0.7729501 ,\n",
       "          0.48457784,   0.45072985],\n",
       "       [  1.70641732,  -8.42368657,  -0.64561534,   7.0063356 ,\n",
       "         -0.99535321,  15.04228213,   8.18603897, -11.66605471,\n",
       "         -7.31753209,  -6.8162875 ],\n",
       "       [  0.9288462 ,  -4.5875671 ,  -0.35099156,   3.81405604,\n",
       "         -0.54296266,   8.18603897,   4.46396405,  -6.3530301 ,\n",
       "         -3.98428568,  -3.70998691],\n",
       "       [ -1.32456017,   6.53652991,   0.50061088,  -5.43539216,\n",
       "          0.7729501 , -11.66605471,  -6.3530301 ,   9.05849914,\n",
       "          5.6780866 ,   5.28812357],\n",
       "       [ -0.83025096,   4.09937494,   0.31418397,  -3.40979621,\n",
       "          0.48457784,  -7.31753209,  -3.98428568,   5.6780866 ,\n",
       "          3.56896295,   3.31645887],\n",
       "       [ -0.77377264,   3.8183708 ,   0.29304502,  -3.17632156,\n",
       "          0.45072985,  -6.8162875 ,  -3.70998691,   5.28812357,\n",
       "          3.31645887,   3.09543957]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_covar= np.stack([np.cov(random_data_gen(n_samples=200, n_feats=10, maha=1., psi_diag=1., ddof=11)[0][:100].T) for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corr = np.stack([(D:=np.diag(1/np.sqrt(np.diag(wishart_cov)))) @ wishart_cov @ D for wishart_cov in data_covar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.        , -0.57665158, -0.56879595, -0.54722413,\n",
       "         -0.56974663, -0.55851872, -0.53493429, -0.5077921 ,\n",
       "         -0.53675823, -0.54514575],\n",
       "        [-0.57665158,  1.        , -0.5450808 , -0.59642015,\n",
       "         -0.54761458, -0.53180432, -0.53460391, -0.55031309,\n",
       "         -0.52889332, -0.58070359],\n",
       "        [-0.56879595, -0.5450808 ,  1.        , -0.55706409,\n",
       "         -0.55097897, -0.55261329, -0.57307214, -0.54386623,\n",
       "         -0.54770076, -0.54206807],\n",
       "        [-0.54722413, -0.59642015, -0.55706409,  1.        ,\n",
       "         -0.54498342, -0.56625302, -0.54996555, -0.58499788,\n",
       "         -0.55194857, -0.55087276],\n",
       "        [-0.56974663, -0.54761458, -0.55097897, -0.54498342,\n",
       "          1.        , -0.54119404, -0.55053714, -0.53831252,\n",
       "         -0.54303238, -0.5525627 ],\n",
       "        [-0.55851872, -0.53180432, -0.55261329, -0.56625302,\n",
       "         -0.54119404,  1.        , -0.52224345, -0.57950635,\n",
       "         -0.53592284, -0.57264009],\n",
       "        [-0.53493429, -0.53460391, -0.57307214, -0.54996555,\n",
       "         -0.55053714, -0.52224345,  1.        , -0.5397973 ,\n",
       "         -0.54697571, -0.58410479],\n",
       "        [-0.5077921 , -0.55031309, -0.54386623, -0.58499788,\n",
       "         -0.53831252, -0.57950635, -0.5397973 ,  1.        ,\n",
       "         -0.55818066, -0.5724972 ],\n",
       "        [-0.53675823, -0.52889332, -0.54770076, -0.55194857,\n",
       "         -0.54303238, -0.53592284, -0.54697571, -0.55818066,\n",
       "          1.        , -0.5567939 ],\n",
       "        [-0.54514575, -0.58070359, -0.54206807, -0.55087276,\n",
       "         -0.5525627 , -0.57264009, -0.58410479, -0.5724972 ,\n",
       "         -0.5567939 ,  1.        ]],\n",
       "\n",
       "       [[ 1.        ,  0.56319349,  0.55431193,  0.54398219,\n",
       "          0.55867628,  0.55159447,  0.52864293,  0.54847856,\n",
       "          0.56542329,  0.54413794],\n",
       "        [ 0.56319349,  1.        ,  0.54872532,  0.56285808,\n",
       "          0.53572369,  0.58511439,  0.55039424,  0.55062206,\n",
       "          0.5656512 ,  0.56170567],\n",
       "        [ 0.55431193,  0.54872532,  1.        ,  0.55073147,\n",
       "          0.56555782,  0.52644314,  0.57000901,  0.5450703 ,\n",
       "          0.53091539,  0.56932276],\n",
       "        [ 0.54398219,  0.56285808,  0.55073147,  1.        ,\n",
       "          0.53566542,  0.54756745,  0.5389124 ,  0.51841271,\n",
       "          0.56086162,  0.54097228],\n",
       "        [ 0.55867628,  0.53572369,  0.56555782,  0.53566542,\n",
       "          1.        ,  0.55741594,  0.55018366,  0.50995776,\n",
       "          0.56480844,  0.58590807],\n",
       "        [ 0.55159447,  0.58511439,  0.52644314,  0.54756745,\n",
       "          0.55741594,  1.        ,  0.58700574,  0.54154033,\n",
       "          0.5481794 ,  0.57963629],\n",
       "        [ 0.52864293,  0.55039424,  0.57000901,  0.5389124 ,\n",
       "          0.55018366,  0.58700574,  1.        ,  0.57855175,\n",
       "          0.52721701,  0.53620511],\n",
       "        [ 0.54847856,  0.55062206,  0.5450703 ,  0.51841271,\n",
       "          0.50995776,  0.54154033,  0.57855175,  1.        ,\n",
       "          0.57016851,  0.5863323 ],\n",
       "        [ 0.56542329,  0.5656512 ,  0.53091539,  0.56086162,\n",
       "          0.56480844,  0.5481794 ,  0.52721701,  0.57016851,\n",
       "          1.        ,  0.55152672],\n",
       "        [ 0.54413794,  0.56170567,  0.56932276,  0.54097228,\n",
       "          0.58590807,  0.57963629,  0.53620511,  0.5863323 ,\n",
       "          0.55152672,  1.        ]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(data_corr, q=[.025, .975], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.        , -0.38559518, -0.38700025, -0.37589341,\n",
       "         -0.38991168, -0.3675221 , -0.37777729, -0.37870946,\n",
       "         -0.36564645, -0.38367836],\n",
       "        [-0.38559518,  1.        , -0.39943433, -0.38765437,\n",
       "         -0.37406198, -0.35338062, -0.36536066, -0.3935133 ,\n",
       "         -0.37319157, -0.3811974 ],\n",
       "        [-0.38700025, -0.39943433,  1.        , -0.41049365,\n",
       "         -0.39463701, -0.38492749, -0.39160884, -0.38795728,\n",
       "         -0.38556426, -0.37863835],\n",
       "        [-0.37589341, -0.38765437, -0.41049365,  1.        ,\n",
       "         -0.38770109, -0.37313139, -0.37616257, -0.41345547,\n",
       "         -0.38100565, -0.395552  ],\n",
       "        [-0.38991168, -0.37406198, -0.39463701, -0.38770109,\n",
       "          1.        , -0.38165204, -0.39418552, -0.37618871,\n",
       "         -0.37721229, -0.40002847],\n",
       "        [-0.3675221 , -0.35338062, -0.38492749, -0.37313139,\n",
       "         -0.38165204,  1.        , -0.37754554, -0.39649449,\n",
       "         -0.34929716, -0.38527781],\n",
       "        [-0.37777729, -0.36536066, -0.39160884, -0.37616257,\n",
       "         -0.39418552, -0.37754554,  1.        , -0.37377921,\n",
       "         -0.37483615, -0.39870899],\n",
       "        [-0.37870946, -0.3935133 , -0.38795728, -0.41345547,\n",
       "         -0.37618871, -0.39649449, -0.37377921,  1.        ,\n",
       "         -0.39281188, -0.40174932],\n",
       "        [-0.36564645, -0.37319157, -0.38556426, -0.38100565,\n",
       "         -0.37721229, -0.34929716, -0.37483615, -0.39281188,\n",
       "          1.        , -0.37380225],\n",
       "        [-0.38367836, -0.3811974 , -0.37863835, -0.395552  ,\n",
       "         -0.40002847, -0.38527781, -0.39870899, -0.40174932,\n",
       "         -0.37380225,  1.        ]],\n",
       "\n",
       "       [[ 1.        ,  0.38420708,  0.37093674,  0.3825018 ,\n",
       "          0.38669297,  0.39425235,  0.34509915,  0.39846959,\n",
       "          0.38033972,  0.38166872],\n",
       "        [ 0.38420708,  1.        ,  0.38234012,  0.4063877 ,\n",
       "          0.39549268,  0.38604442,  0.40674387,  0.39266133,\n",
       "          0.35152567,  0.40590646],\n",
       "        [ 0.37093674,  0.38234012,  1.        ,  0.36459989,\n",
       "          0.39418119,  0.36794394,  0.40451839,  0.35973225,\n",
       "          0.35724721,  0.39025123],\n",
       "        [ 0.3825018 ,  0.4063877 ,  0.36459989,  1.        ,\n",
       "          0.37225688,  0.40254786,  0.38497753,  0.39773714,\n",
       "          0.3943761 ,  0.37736504],\n",
       "        [ 0.38669297,  0.39549268,  0.39418119,  0.37225688,\n",
       "          1.        ,  0.36583157,  0.37423126,  0.37456532,\n",
       "          0.39785179,  0.38427822],\n",
       "        [ 0.39425235,  0.38604442,  0.36794394,  0.40254786,\n",
       "          0.36583157,  1.        ,  0.41096432,  0.38811516,\n",
       "          0.37054534,  0.40658096],\n",
       "        [ 0.34509915,  0.40674387,  0.40451839,  0.38497753,\n",
       "          0.37423126,  0.41096432,  1.        ,  0.41712234,\n",
       "          0.37343456,  0.38299879],\n",
       "        [ 0.39846959,  0.39266133,  0.35973225,  0.39773714,\n",
       "          0.37456532,  0.38811516,  0.41712234,  1.        ,\n",
       "          0.37635428,  0.38751419],\n",
       "        [ 0.38033972,  0.35152567,  0.35724721,  0.3943761 ,\n",
       "          0.39785179,  0.37054534,  0.37343456,  0.37635428,\n",
       "          1.        ,  0.39872017],\n",
       "        [ 0.38166872,  0.40590646,  0.39025123,  0.37736504,\n",
       "          0.38427822,  0.40658096,  0.38299879,  0.38751419,\n",
       "          0.39872017,  1.        ]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(data_corr, q=[.1, .9], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BROKEN: Kronecker product version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def comm_mat(m, n):\n",
    "    # determine permutation applied by K\n",
    "    w = np.arange(m * n).reshape((m, n), order=\"F\").T.ravel(order=\"F\")\n",
    "    # apply this permutation to the rows (i.e. to each column) of identity matrix and return result\n",
    "    return np.eye(m * n)[w, :]\n",
    "\n",
    "def vec(X):\n",
    "    return np.ravel(X, order='F')\n",
    "\n",
    "def kron_Var_invwish(psi, dof):\n",
    "    p = len(psi)\n",
    "    c2 = ((dof - p)*(dof - p - 1)*(dof - p - 3))**(-1)\n",
    "    c1 = (dof-p-2)*c2\n",
    "    c3 = (dof-p-1)**(-2)\n",
    "    K_pp = comm_mat(p, p)\n",
    "    return c1 * np.kron(psi, psi) + c2*vec(psi) @ vec(psi).T + c2 * K_pp @ np.kron(psi, psi) \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a3043ce43b8894bffef93d1839881ad472cb7607c1a3c9f1cfc63c042591dd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
